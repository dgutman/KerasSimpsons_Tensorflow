{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# A multi classes image classifier, based on convolutional neural network using Keras and Tensorflow. \n",
    "# A multi-label classifier (having one fully-connected layer at the end), with multi-classification (18 classes, in this instance).\n",
    "# Largely copied from the code https://gist.github.com/seixaslipe\n",
    "# This is based on these posts: https://medium.com/alex-attia-blog/the-simpsons-character-recognition-using-keras-d8e1796eae36\n",
    "# Data downloaded from Kaggle \n",
    "# Will emulate the image classification functionlities for Neuro Pathology images/slides (WSI-Whole Slide images)\n",
    "# Will implement/include data manipulating functionalities based on Girder (https://girder.readthedocs.io/en/latest/)\n",
    "# Has 6 convulsions, filtering start with 64, 128, 256 with flattening to 1024\n",
    "# Used Keras.ImageDataGenerator for Training/Validation data augmentation and the augmented images are flown from respective directory\n",
    "# Environment: A docker container having Keras, TensorFlow, Python-2 with GPU based execution\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "import datetime, time, os, sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib as plt\n",
    "plt.use('Agg')\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.figure\n",
    "import pickle \n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "import nvidia_smi as nvs\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nvidia-ml-py --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver Version: 390.77\n",
      "Device 0: TITAN V\n",
      "Found 19548 images belonging to 20 classes.\n",
      "Found 990 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "modelInfo = {}\n",
    "modelInfo['Device']  = {} ## Initialize an object to store info on the model and time info\n",
    "\n",
    "nvs.nvmlInit()\n",
    "\n",
    "driverVersion = nvs.nvmlSystemGetDriverVersion()\n",
    "print(\"Driver Version: {}\".format(driverVersion))\n",
    "modelInfo['Device']['driverVersion']  = driverVersion\n",
    "\n",
    "# e.g. will print:\n",
    "#   Driver Version: 352.00\n",
    "deviceCount = nvs.nvmlDeviceGetCount()\n",
    "deviceNames = []\n",
    "for i in range(deviceCount):\n",
    "    handle = nvs.nvmlDeviceGetHandleByIndex(i)\n",
    "    dvn = nvs.nvmlDeviceGetName(handle) # store the device name\n",
    "    print(\"Device {}: {}\".format(i,  dvn))\n",
    "    deviceNames.append(dvn)\n",
    "    # e.g. will print:\n",
    "    #  Device 0 : Tesla K40c\n",
    "nvs.nvmlShutdown()\n",
    "\n",
    "modelInfo['Device']['deviceNames']  = deviceNames\n",
    "\n",
    "\n",
    "### These parameters can be tuned and may affect classification results or accuracy\n",
    "img_width, img_height = 64, 64\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "modelInfo['batch_size'] = batch_size\n",
    "modelInfo['epochs'] = epochs\n",
    "modelInfo['img_width'] = 64\n",
    "modelInfo['img_height'] = 64\n",
    " \n",
    "\n",
    "### Define input dirs and output for results which contain the models as well as stats on the run\n",
    "train_data_dir = '/data/train' \n",
    "validation_data_dir = '/data/test' \n",
    "\n",
    "resultsDir =\"/app/results/\"\n",
    "if not os.path.isdir(resultsDir):\n",
    "    os.makedirs(resultsDir)\n",
    "\n",
    "nb_train_samples = 0\n",
    "\n",
    "for root, dirs, files in os.walk(train_data_dir):\n",
    "    nb_train_samples += len(files)\n",
    "\n",
    "nb_validation_samples = 0\n",
    "for root, dirs, files in os.walk(validation_data_dir):\n",
    "    nb_validation_samples += len(files)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Only rescaling for validation\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255.0)\n",
    "\n",
    "# Flows the data directly from the directory structure, resizing where needed\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "NumLabels = len(validation_generator.class_indices)\n",
    "\n",
    "'''\n",
    "6-conv layers - added on 06/21, Raj\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NumLabels, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Captures GPU usage\n",
    "#subprocess.Popen(\"timeout 120 nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv -l 1 | sed s/%//g > /app/results/GPU-stats.log\",shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/305 [=================>............] - ETA: 1:08 - loss: 2.8258 - acc: 0.1238"
     ]
    }
   ],
   "source": [
    "# Timehistory callback to get epoch run times\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Model fitting and training run\n",
    "simpsonsModel = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "\n",
    "    callbacks=[time_callback])    \n",
    "\n",
    "\n",
    "print \"Finished running the basic model... trying to save results now..\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write the each epoch run time into a json file\n",
    "now = datetime.datetime.now()\n",
    "filetime = str(now.year)+str(now.month)+str(now.day)+'_'+str(now.hour)+str(now.minute)\n",
    "\n",
    "modelInfo['epochTimeInfo'] = time_callback.times\n",
    "\n",
    "\n",
    "## Write out the h5/model\n",
    "modelfilename=resultsDir+'Simpsonsmodel_'+filetime+'.h5'\n",
    "model.save(modelfilename)\n",
    "\n",
    "## This outputs the training and validation accuracy and loss functions for each epoch\n",
    "## This will be graphed as well using plotly ... you can use this data to look for overfitting\n",
    "## and/or when you can stop training your model because it stops improving\n",
    "modelInfo['historyData'] =  pd.DataFrame(simpsonsModel.history).to_dict(orient='records')\n",
    "\n",
    "###target_names maps the character names (or labels) to the index(integer) used in the output files\n",
    "modelInfo['target_names']  = validation_generator.class_indices\n",
    "\n",
    "modelInfo['labelname_to_index']  = validation_generator.class_indices\n",
    "modelInfo['index_to_labelname']  = {(v,k) for k,v in validation_generator.class_indices.iteritems() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = validation_generator.class_indices\n",
    "\n",
    "## Prediction for TEST data set\n",
    "TRAIN_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "## This yields a probability for the TEST data set of the likelihood a given image is a member of a class\n",
    "\n",
    "##Prediction for TEST data set... this is the best guess i.e. highest matching class/label\n",
    "TRAIN_pred_label = np.argmax(TRAIN_pred, axis=1)\n",
    "\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, TRAIN_pred_label)\n",
    "cls_rpt = classification_report(validation_generator.classes, TRAIN_pred_label, target_names=target_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "charles_montgomery_burns       0.15      0.10      0.12        48\n",
      "            ned_flanders       0.06      0.06      0.06        50\n",
      "           homer_simpson       0.00      0.00      0.00        50\n",
      "           lenny_leonard       0.04      0.06      0.05        48\n",
      "  abraham_grampa_simpson       0.12      0.14      0.13        50\n",
      "            mayor_quimby       0.09      0.06      0.07        49\n",
      "            chief_wiggum       0.02      0.02      0.02        50\n",
      "          edna_krabappel       0.04      0.04      0.04        50\n",
      "  apu_nahasapeemapetilon       0.02      0.02      0.02        50\n",
      "       principal_skinner       0.07      0.08      0.07        50\n",
      "           marge_simpson       0.12      0.02      0.03        50\n",
      "             moe_szyslak       0.04      0.06      0.05        50\n",
      "            nelson_muntz       0.08      0.08      0.08        50\n",
      "        krusty_the_clown       0.05      0.02      0.03        50\n",
      "           kent_brockman       0.09      0.10      0.09        49\n",
      "            bart_simpson       0.08      0.10      0.09        50\n",
      "            sideshow_bob       0.06      0.08      0.07        49\n",
      "          comic_book_guy       0.11      0.02      0.03        50\n",
      "            lisa_simpson       0.09      0.18      0.12        50\n",
      "     milhouse_van_houten       0.02      0.02      0.02        47\n",
      "\n",
      "             avg / total       0.07      0.06      0.06       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "charles_montgomery_burns       0.07      0.06      0.07        48\n",
      "            ned_flanders       0.07      0.06      0.06        50\n",
      "           homer_simpson       0.13      0.10      0.11        50\n",
      "           lenny_leonard       0.06      0.04      0.05        48\n",
      "  abraham_grampa_simpson       0.08      0.12      0.10        50\n",
      "            mayor_quimby       0.14      0.06      0.09        49\n",
      "            chief_wiggum       0.14      0.08      0.10        50\n",
      "          edna_krabappel       0.05      0.10      0.07        50\n",
      "  apu_nahasapeemapetilon       0.02      0.02      0.02        50\n",
      "       principal_skinner       0.02      0.02      0.02        50\n",
      "           marge_simpson       0.05      0.04      0.05        50\n",
      "             moe_szyslak       0.05      0.04      0.04        50\n",
      "            nelson_muntz       0.02      0.02      0.02        50\n",
      "        krusty_the_clown       0.00      0.00      0.00        50\n",
      "           kent_brockman       0.04      0.04      0.04        49\n",
      "            bart_simpson       0.05      0.12      0.07        50\n",
      "            sideshow_bob       0.04      0.04      0.04        49\n",
      "          comic_book_guy       0.12      0.04      0.06        50\n",
      "            lisa_simpson       0.05      0.06      0.05        50\n",
      "     milhouse_van_houten       0.03      0.04      0.04        47\n",
      "\n",
      "             avg / total       0.06      0.06      0.05       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "charles_montgomery_burns       0.05      0.04      0.05        48\n",
      "            ned_flanders       0.10      0.10      0.10        50\n",
      "           homer_simpson       0.05      0.06      0.06        50\n",
      "           lenny_leonard       0.04      0.04      0.04        48\n",
      "  abraham_grampa_simpson       0.09      0.10      0.09        50\n",
      "            mayor_quimby       0.00      0.00      0.00        49\n",
      "            chief_wiggum       0.03      0.02      0.02        50\n",
      "          edna_krabappel       0.02      0.02      0.02        50\n",
      "  apu_nahasapeemapetilon       0.06      0.06      0.06        50\n",
      "       principal_skinner       0.06      0.06      0.06        50\n",
      "           marge_simpson       0.04      0.04      0.04        50\n",
      "             moe_szyslak       0.02      0.02      0.02        50\n",
      "            nelson_muntz       0.10      0.10      0.10        50\n",
      "        krusty_the_clown       0.06      0.06      0.06        50\n",
      "           kent_brockman       0.04      0.04      0.04        49\n",
      "            bart_simpson       0.04      0.04      0.04        50\n",
      "            sideshow_bob       0.04      0.04      0.04        49\n",
      "          comic_book_guy       0.07      0.06      0.06        50\n",
      "            lisa_simpson       0.06      0.06      0.06        50\n",
      "     milhouse_van_houten       0.05      0.04      0.04        47\n",
      "\n",
      "             avg / total       0.05      0.05      0.05       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)  ### This is a 20 by 20 matrix\n",
    "\n",
    "## This looks cool, but we need to turn it into a table I guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turning into classification report into classification object\n",
    "avgresults = cls_rpt.strip().split('\\n')[-1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallResults={'label' : 'avg/total', 'precision': avgresults[3], 'recall':avgresults[4],'f1-score':avgresults[5], 'support':avgresults[6]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support  =  precision_recall_fscore_support(validation_generator.classes, TEST_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['classificationObject'] =  characterResultsArray =  {\n",
    "    'label': validation_generator.class_indices.keys(),\n",
    "    'precision': precision,\n",
    "    'recall':recall,\n",
    "    'fscore': fscore, 'support':support,\n",
    "    'overallResults':{'label' : 'avg/total', \n",
    "                      'precision': avgresults[3], \n",
    "                      'recall':avgresults[4],\n",
    "                      'f1-score':avgresults[5],\n",
    "                      'support':avgresults[6]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['classificationObject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['confusion_matrix'] = confusion_matrix(validation_generator.classes, TEST_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['confusion_matrix']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   filename --- CLASS\n",
    "\n",
    "\n",
    "### LAST BUT NOT LEAST --- \n",
    "\n",
    "\n",
    "\n",
    "# MAKE IT A PARAMETER OUTPUT MODELPREDICTIOJ FOR TRAIN AND TEST OR JUST TEST  \n",
    "\n",
    "# for image in glob.glob('/data/train/*/'):\n",
    "\n",
    "#     I WANT\n",
    "    \n",
    "#     ['filename': \"somename\", 'actualImageLabel': asIndex, 'modelPrection': X ]\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "dgWant = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld\n",
    "    for img in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        imgPath = \"/data/test/%s/%s\" % (fld, img)\n",
    "        x = image.load_img(imgPath, target_size=(64,64))\n",
    "        x = image.img_to_array(x)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        x = x/255.\n",
    "        pr=model.predict(x)\n",
    "        curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr} \n",
    "        dgWant.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer_simpson'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dgWant[300]['modelprediction']\n",
    "maxIndex = np.argmax(d)\n",
    "\n",
    "dict(modelInfo['index_to_labelname'])[maxIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(modelInfo['index_to_labelname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actualImageLabel': 'chief_wiggum',\n",
       " 'filename': 'chief_wiggum_27.jpg',\n",
       " 'modelprediction': array([[4.29841629e-09, 1.28591315e-09, 1.29947765e-07, 2.76901956e-05,\n",
       "         9.99919176e-01, 2.25431722e-06, 1.29766701e-11, 5.24551024e-06,\n",
       "         8.81407658e-09, 4.40973110e-07, 4.12774710e-07, 1.00335065e-07,\n",
       "         1.16415742e-06, 3.22805960e-07, 8.68021687e-07, 4.18807031e-05,\n",
       "         1.47505763e-09, 1.46813672e-08, 2.85744591e-07, 3.95747257e-08]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgWant[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
