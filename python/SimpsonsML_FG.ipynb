{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All code from Raj and Dr. Gutman\n",
    "## minor FG changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# A multi-class image classifier, based on convolutional neural network using Keras and Tensorflow. \n",
    "# 20 classes\n",
    "# Largely copied from: https://gist.github.com/seixaslipe\n",
    "# Based on: https://medium.com/alex-attia-blog/the-simpsons-character-recognition-using-keras-d8e1796eae36\n",
    "# Data downloaded from Kaggle \n",
    "# Will emulate the image classification functionlities for Neuro Pathology images/slides (WSI-Whole Slide images)\n",
    "# Will implement/include data manipulating functionalities based on Girder (https://girder.readthedocs.io/en/latest/)\n",
    "# Has 6 convolutions, filtering:64, 128, 256 with flattening to 1024\n",
    "# Keras.ImageDataGenerator for Training/Validation data augmentation\n",
    "# Environment: Keras, TensorFlow, Python-2, GPU-enabled\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "import datetime, time, os, sys\n",
    "import numpy as np\n",
    "import h5py, json\n",
    "import matplotlib as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "import nvidia_smi as nvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add in ability to add memory as needed and not preallocate all GPU RAM--will allow parallel models to be run\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata json: GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelinfo: json to store system metadata:\n",
    "modelInfo = {}\n",
    "# GPU/CPU:\n",
    "modelInfo['Device']  = {}\n",
    "\n",
    "# initialize GPU to get detailed info:\n",
    "nvs.nvmlInit()\n",
    "# Driver version:\n",
    "driverVersion = nvs.nvmlSystemGetDriverVersion()\n",
    "# Number of devices:\n",
    "deviceCount = nvs.nvmlDeviceGetCount()\n",
    "# Device Names:\n",
    "deviceNames = []\n",
    "for i in range(deviceCount):\n",
    "    handle = nvs.nvmlDeviceGetHandleByIndex(i)\n",
    "    dvn = nvs.nvmlDeviceGetName(handle) # store the device name\n",
    "    deviceNames.append(dvn)\n",
    "    # e.g. will print:\n",
    "    #  Device 0 : Tesla K40c\n",
    "nvs.nvmlShutdown()\n",
    "# Save GPU metadata to modelInfo\n",
    "modelInfo['Device']['driverVersion']  = driverVersion\n",
    "modelInfo['Device']['deviceNames']  = deviceNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimension:\n",
    "img_width, img_height = 64, 64\n",
    "# Epochs\n",
    "epochs = 30\n",
    "# Batch size:\n",
    "batch_size = 32\n",
    "\n",
    "# Save model metadata to modelInfo:\n",
    "modelInfo['batch_size'] = batch_size\n",
    "modelInfo['epochs'] = epochs\n",
    "modelInfo['img_width'] = 64\n",
    "modelInfo['img_height'] = 64\n",
    " \n",
    "\n",
    "# Training and Testing Images Locations\n",
    "training_dir = '/data/train'\n",
    "validation_dir = '/data/test'\n",
    "testing_dir = '/data/test' ###### WARNING: This should be changed once we get Testing Images\n",
    "\n",
    "# Results Location:\n",
    "results_dir =\"/output/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count training images:\n",
    "ntraining = 0\n",
    "for root, dirs, files in os.walk(training_dir):\n",
    "    ntraining += len(files)\n",
    "\n",
    "# Count validation images:\n",
    "nvalidation = 0\n",
    "for root, dirs, files in os.walk(validation_dir):\n",
    "    nvalidation += len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19548 images belonging to 20 classes.\n",
      "Found 990 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "# get data format:\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "# Training Image Augmentation:\n",
    "# -Scale\n",
    "# -Shear\n",
    "# -Zoom\n",
    "# -Height and Width Shift\n",
    "# -Fill: Nearest\n",
    "# -Horizontal Flip\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Validation Image Augmentation:\n",
    "# -Scale\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255.0)\n",
    "\n",
    "# Training Image Generator:\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Validation Image Generator:\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Number of Classes/Labels:\n",
    "nLabels = len(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# - 6 Convolusional Layers\n",
    "# - RELU Activation\n",
    "# 32 -> 64 -> 256 -> 1024\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nLabels, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Captures GPU usage\n",
    "#subprocess.Popen(\"timeout 120 nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv -l 1 | sed s/%//g > /app/results/GPU-stats.log\",shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeHistory: Callback class to get timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timehistory callback to get epoch run times\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "610/610 [==============================] - 152s 248ms/step - loss: 2.7736 - acc: 0.1406 - val_loss: 2.5836 - val_acc: 0.2167\n",
      "Epoch 2/30\n",
      "610/610 [==============================] - 150s 246ms/step - loss: 2.1174 - acc: 0.3537 - val_loss: 1.7135 - val_acc: 0.4854\n",
      "Epoch 3/30\n",
      "610/610 [==============================] - 151s 247ms/step - loss: 1.5420 - acc: 0.5381 - val_loss: 1.1640 - val_acc: 0.6521\n",
      "Epoch 4/30\n",
      "610/610 [==============================] - 149s 245ms/step - loss: 1.1840 - acc: 0.6491 - val_loss: 0.8457 - val_acc: 0.7656\n",
      "Epoch 5/30\n",
      "610/610 [==============================] - 149s 244ms/step - loss: 0.9790 - acc: 0.7100 - val_loss: 0.6761 - val_acc: 0.8167\n",
      "Epoch 6/30\n",
      "610/610 [==============================] - 149s 244ms/step - loss: 0.8752 - acc: 0.7455 - val_loss: 0.5768 - val_acc: 0.8302\n",
      "Epoch 7/30\n",
      "201/610 [========>.....................] - ETA: 1:36 - loss: 0.7754 - acc: 0.7727"
     ]
    }
   ],
   "source": [
    "# Model fitting and training run\n",
    "simpsonsModel = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch= ntraining // batch_size,\n",
    "    epochs= epochs,\n",
    "    validation_data= validation_generator,\n",
    "    validation_steps= nvalidation // batch_size,\n",
    "\n",
    "    callbacks= [time_callback]\n",
    ")    \n",
    "\n",
    "print \"Training Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Run metadata to modelInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Get timestamp:\n",
    "now = datetime.datetime.now()\n",
    "filetime = str(now.year)+str(now.month)+str(now.day)+'_'+str(now.hour)+str(now.minute)\n",
    "\n",
    "# Time per Epoch:\n",
    "modelInfo['epochTimeInfo'] = time_callback.times\n",
    "\n",
    "# Save timestamped model to modelfilename\n",
    "modelfilename=results_dir+'Simpsonsmodel_'+filetime+'.h5'\n",
    "model.save(modelfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Run Results to modelInfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation accuracy and loss per epoch\n",
    "modelInfo['historyData'] =  pd.DataFrame(simpsonsModel.history).to_dict(orient='records')\n",
    "\n",
    "###target_names maps the character names (or labels) to the index(integer) used in the output files\n",
    "modelInfo['target_names']  = validation_generator.class_indices\n",
    "\n",
    "modelInfo['labelname_to_index']  = validation_generator.class_indices\n",
    "modelInfo['index_to_labelname']  = dict({(v,k) for k,v in validation_generator.class_indices.iteritems() })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model on Test Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of Testing Images\n",
    "ntesting = 0\n",
    "for root, dirs, files in os.walk(testing_dir):\n",
    "    ntesting += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 990 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "# Validation Image Generator:\n",
    "testing_generator_noShuffle = valid_datagen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_Validation: narray\n",
    "# row= image\n",
    "# column= probability of falling within label matching column_index\n",
    "predict_Testing = model.predict_generator(testing_generator_noShuffle, ntesting // batch_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label:Index Dictionary\n",
    "label_index_dict = testing_generator_noShuffle.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Prediction for all labels: I don't know why we are calculating this (FG)\n",
    "best_prediction_per_label= [ max( predict_Testing[:,j] ) for j in range( predict_Testing.shape[1] ) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted label for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels= []\n",
    "# Find highest probability in prediction list for each image\n",
    "for i in predict_Testing:\n",
    "    i= list(i)\n",
    "    max_value = max(i) \n",
    "    predicted_labels.append( i.index(max_value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(testing_generator_noShuffle.classes, predicted_labels)\n",
    "cls_rpt = classification_report(testing_generator_noShuffle.classes, predicted_labels, target_names= testing_generator_noShuffle.class_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "charles_montgomery_burns       0.96      0.90      0.92        48\n",
      "            ned_flanders       1.00      1.00      1.00        50\n",
      "           homer_simpson       0.92      0.92      0.92        50\n",
      "           lenny_leonard       0.96      0.96      0.96        48\n",
      "  abraham_grampa_simpson       0.94      0.94      0.94        50\n",
      "            mayor_quimby       0.96      0.88      0.91        49\n",
      "            chief_wiggum       1.00      0.92      0.96        50\n",
      "          edna_krabappel       0.75      0.92      0.83        50\n",
      "  apu_nahasapeemapetilon       0.98      0.96      0.97        50\n",
      "       principal_skinner       0.96      0.98      0.97        50\n",
      "           marge_simpson       1.00      0.94      0.97        50\n",
      "             moe_szyslak       0.92      0.92      0.92        50\n",
      "            nelson_muntz       0.91      0.96      0.93        50\n",
      "        krusty_the_clown       0.98      0.92      0.95        50\n",
      "           kent_brockman       0.98      0.98      0.98        49\n",
      "            bart_simpson       0.92      0.98      0.95        50\n",
      "            sideshow_bob       0.92      0.98      0.95        49\n",
      "          comic_book_guy       0.96      0.88      0.92        50\n",
      "            lisa_simpson       0.98      0.98      0.98        50\n",
      "     milhouse_van_houten       0.98      1.00      0.99        47\n",
      "\n",
      "             avg / total       0.95      0.95      0.95       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cls_rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF FG Checking (7/4/2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turning into classification report into classification object\n",
    "avgresults = cls_rpt.strip().split('\\n')[-1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overallResults={'label' : 'avg/total', 'precision': list(avgresults[3]), 'recall':list(avgresults[4]),'f1-score':list(avgresults[5]), 'support':list(avgresults[6])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['classificationObject'] =  characterResultsArray =  {\n",
    "    'label': validation_generator.class_indices.keys(),\n",
    "    'precision': precision.tolist(),\n",
    "    'recall':recall.tolist(),\n",
    "    'fscore': fscore.tolist(), 'support':support.tolist(),\n",
    "    'overallResults':{'label' : 'avg/total', \n",
    "                      'precision': avgresults[3], \n",
    "                      'recall':avgresults[4],\n",
    "                      'f1-score':avgresults[5],\n",
    "                      'support':avgresults[6]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support  =  precision_recall_fscore_support(validation_generator.classes, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fscore': array([0.92473118, 1.        , 0.92      , 0.95833333, 0.94      ,\n",
       "        0.91489362, 0.95833333, 0.82882883, 0.96969697, 0.97029703,\n",
       "        0.96907216, 0.92      , 0.93203883, 0.94845361, 0.97959184,\n",
       "        0.95145631, 0.95049505, 0.91666667, 0.98      , 0.98947368]),\n",
       " 'label': ['charles_montgomery_burns',\n",
       "  'ned_flanders',\n",
       "  'homer_simpson',\n",
       "  'lenny_leonard',\n",
       "  'abraham_grampa_simpson',\n",
       "  'mayor_quimby',\n",
       "  'chief_wiggum',\n",
       "  'edna_krabappel',\n",
       "  'apu_nahasapeemapetilon',\n",
       "  'principal_skinner',\n",
       "  'marge_simpson',\n",
       "  'moe_szyslak',\n",
       "  'nelson_muntz',\n",
       "  'krusty_the_clown',\n",
       "  'kent_brockman',\n",
       "  'bart_simpson',\n",
       "  'sideshow_bob',\n",
       "  'comic_book_guy',\n",
       "  'lisa_simpson',\n",
       "  'milhouse_van_houten'],\n",
       " 'overallResults': {'f1-score': [u'0', u'.', u'9', u'5'],\n",
       "  'label': 'avg/total',\n",
       "  'precision': [u'0', u'.', u'9', u'5'],\n",
       "  'recall': [u'0', u'.', u'9', u'5'],\n",
       "  'support': [u'9', u'9', u'0']},\n",
       " 'precision': array([0.95555556, 1.        , 0.92      , 0.95833333, 0.94      ,\n",
       "        0.95555556, 1.        , 0.75409836, 0.97959184, 0.96078431,\n",
       "        1.        , 0.92      , 0.90566038, 0.9787234 , 0.97959184,\n",
       "        0.9245283 , 0.92307692, 0.95652174, 0.98      , 0.97916667]),\n",
       " 'recall': array([0.89583333, 1.        , 0.92      , 0.95833333, 0.94      ,\n",
       "        0.87755102, 0.92      , 0.92      , 0.96      , 0.98      ,\n",
       "        0.94      , 0.92      , 0.96      , 0.92      , 0.97959184,\n",
       "        0.98      , 0.97959184, 0.88      , 0.98      , 1.        ]),\n",
       " 'support': array([48, 50, 50, 48, 50, 49, 50, 50, 50, 50, 50, 50, 50, 50, 49, 50, 49,\n",
       "        50, 50, 47])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelInfo['classificationObject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInfo['confusion_matrix'] = confusion_matrix(validation_generator.classes, predicted_labels).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testPredictionData = []\n",
    "\n",
    "model=load_model(modelfilename)\n",
    "\n",
    "idx_to_lbl = dict(modelInfo['index_to_labelname'])\n",
    "\n",
    "for fld in os.listdir('/data/test/'): \n",
    "    trueLabel = fld\n",
    "    for img in os.listdir('/data/test/%s/' %trueLabel): \n",
    "        imgPath = \"/data/test/%s/%s\" % (fld, img)\n",
    "        x = image.load_img(imgPath, target_size=(64,64))\n",
    "        x = image.img_to_array(x)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        x = x/255.\n",
    "        pr=model.predict(x)\n",
    "        \n",
    "        curr = {'filename': img, 'actualImageLabel': fld, 'modelprediction':pr.tolist(), 'predictionAcc': float(pr.max()),\n",
    "                   'predictedImageLabel': idx_to_lbl[np.argmax(pr)]} \n",
    "        testPredictionData.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "modelInfo['testPredictionData'] = testPredictionData\n",
    "\n",
    "modelOutputData = os.path.join(results_dir,'modelRunInfo.'+filetime+'.json')\n",
    "\n",
    "# modelInfo['target_names']  = validation_generator.class_indices\n",
    "\n",
    "# modelInfo['labelname_to_index']  = validation_generator.class_indices\n",
    "# modelInfo['index_to_labelname']  = {(v,k) for k,v in validation_generator.class_indices.iteritems() }\n",
    "with open(modelOutputData,\"w\") as fp:\n",
    "    json.dump(modelInfo,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpsonsmodel_201886_1936.h5  modelRunInfo.201886_211.json\r\n",
      "Simpsonsmodel_201886_211.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls /output/results\n",
    "!cp /output/results/* /app/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change modelPredictionoutput to 4 or 5 digts.. not 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelInfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99cbb82c37f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testPredictionData'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modelprediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'modelInfo' is not defined"
     ]
    }
   ],
   "source": [
    "modelInfo['testPredictionData'][0]['modelprediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
